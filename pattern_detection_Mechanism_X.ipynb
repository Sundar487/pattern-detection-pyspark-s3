{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a0f7daf-c868-47ad-b63f-14a2ac92c551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e7dfb4-0be5-4209-9e6f-0440603594a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "file_id = \"1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\"\n",
    "download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
    "\n",
    "dbfs_path = \"/dbfs/FileStore/tables/\"\n",
    "file_path = os.path.join(dbfs_path, \"transactions.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(dbfs_path, exist_ok=True)\n",
    "\n",
    "# Download the file and write it\n",
    "response = requests.get(download_url)\n",
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(f\"File downloaded to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e861b06-b06b-4e0f-b629-12a2e6b5fdd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions  = spark.read.csv('/FileStore/tables/transactions.csv', header = True) \n",
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b7941f-61d6-4b5f-ae6f-b6d834e9418e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "df_transactions = (\n",
    "    df_transactions\n",
    "    .withColumn(\"step\", regexp_replace(col(\"step\"), \"'\", \"\").cast(\"int\"))\n",
    "    .withColumn(\"amount\", regexp_replace(col(\"amount\"), \"'\", \"\").cast(\"double\"))\n",
    "    .withColumn(\"fraud\", regexp_replace(col(\"fraud\"), \"'\", \"\").cast(\"int\"))\n",
    "    .withColumn(\"age\", regexp_replace(col(\"age\"), \"'\", \"\").cast(\"int\"))\n",
    "    .withColumn(\"customer\", regexp_replace(col(\"customer\"), \"'\", \"\"))\n",
    "    .withColumn(\"gender\", regexp_replace(col(\"gender\"), \"'\", \"\"))\n",
    "    .withColumn(\"zipcodeOri\", regexp_replace(col(\"zipcodeOri\"), \"'\", \"\"))\n",
    "    .withColumn(\"merchant\", regexp_replace(col(\"merchant\"), \"'\", \"\"))\n",
    "    .withColumn(\"zipMerchant\", regexp_replace(col(\"zipMerchant\"), \"'\", \"\"))\n",
    "    .withColumn(\"category\", regexp_replace(col(\"category\"), \"'\", \"\"))\n",
    ")\n",
    "\n",
    "df_transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25568ac0-33ca-4090-8540-725e9f941fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bfb4337-76d5-48b5-af0e-31ad8c0f73cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '***************'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '**************'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbcdc394-77a7-46eb-bbb1-41348c331dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(region_name = 'ap-south-1',aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "print('successfully aws session created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75e9071-482b-4d06-840d-82f11cef099f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name, region=\"ap-south-1\"):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bucket\n",
    "    try:\n",
    "        if region is None:\n",
    "            s3_client = boto3.client('s3')\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            s3_client = boto3.client('s3', region_name=region)\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59326e94-8ef8-445b-844d-d86a6f1b3136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "\n",
    "existing_buckets = []\n",
    "for bucket in response['Buckets']:\n",
    "    existing_buckets.append(bucket[\"Name\"])\n",
    "\n",
    "if \"pattern-detection-pyspark\" not in existing_buckets: \n",
    "    create_bucket(\"pattern-detection-pyspark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "852d2e34-84f8-436a-b126-741eeb5a4b55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Set up the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the bucket and folder name\n",
    "bucket_name = 'pattern-detection-pyspark'\n",
    "folder_name = 'streaming/input/'\n",
    "\n",
    "# Create the folder (prefix)\n",
    "response = s3.put_object(Bucket=bucket_name, Key=folder_name)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aceec248-350e-458d-a981-14b6140de9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "bucket_name = \"pattern-detection-pyspark\"\n",
    "s3_folder = \"streaming/input/\"\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d28ac3a-eff8-4515-a19a-587ecd1980e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6a50b3-4e00-40a9-851d-7b1f8f1fa5ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "df_transactions_id = df_transactions.withColumn(\"row_id\", row_number().over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba620278-a1c1-4cc1-b21f-056db905c522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions_id = df_transactions_id.limit(1000) \n",
    "df_transactions_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3455343b-81b7-4273-9e18-de1f66a64658",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "\n",
    "window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "df_with_id = df_transactions_id.withColumn(\"row_id\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "chunk_size = 200\n",
    "total_rows = df_with_id.count()\n",
    "total_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "\n",
    "for chunk_id in range(total_chunks): \n",
    "    lower = chunk_id * chunk_size + 1\n",
    "    upper = lower + chunk_size\n",
    "\n",
    "    df_chunk = (\n",
    "        df_with_id.filter((df_with_id.row_id >= lower) & (df_with_id.row_id < upper)).drop(\"row_id\")\n",
    "    )\n",
    "\n",
    "    output_path = f\"s3a://{bucket_name}/{s3_folder}chunk_{chunk_id}.csv\"\n",
    "\n",
    "    df_chunk.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .csv(output_path)\n",
    "\n",
    "    print(f\" Uploaded chunk_{chunk_id}.csv to {output_path}\")\n",
    "    time.sleep(1)  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pattern_detection_Mechanism_X",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
