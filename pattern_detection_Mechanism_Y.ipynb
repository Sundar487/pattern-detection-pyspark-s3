{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d87fb7a8-4fab-4b1b-b1bd-08b79d88f4d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '***************'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '**************'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "578e0fa2-f42f-41e4-828f-39363af26352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.Session(region_name = 'ap-south-1',aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "                    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "print('successfully aws session created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26a71518-d201-4f26-9c4f-01f33f894050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "bucket_name = \"pattern-detection-pyspark\"\n",
    "s3_folder = \"streaming/input/\"\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f2b9b7-58ce-4c24-a47d-703a495d577d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "transaction_schema = (\n",
    "    StructType()\n",
    "    .add(\"step\", IntegerType())\n",
    "    .add(\"customer\", StringType())\n",
    "    .add(\"age\", IntegerType())\n",
    "    .add(\"gender\", StringType())\n",
    "    .add(\"zipcodeOri\", StringType())\n",
    "    .add(\"merchant\", StringType())\n",
    "    .add(\"zipMerchant\", StringType())\n",
    "    .add(\"category\", StringType())\n",
    "    .add(\"amount\", DoubleType())\n",
    "    .add(\"fraud\", IntegerType())\n",
    "    .add(\"weight\", DoubleType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a520b809-0a4f-41a0-bac9-89c7bbab1905",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78f341b-ad9a-411e-9cb3-59619dc97c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, avg, count, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# ------------------ Spark Initialization ------------------\n",
    "spark = SparkSession.builder.appName(\"Pattern-Detection-Pipeline\").getOrCreate()\n",
    "\n",
    "# ------------------ Configuration ------------------\n",
    "bucket_name = \"pattern-detection-pyspark\"\n",
    "input_prefix = \"streaming/input/\"\n",
    "output_prefix = \"streaming/output/\"\n",
    "y_start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "pg_conn = psycopg2.connect(\n",
    "    host=\"database-1.cl80waky6nkw.ap-south-1.rds.amazonaws.com\",\n",
    "    database=\"pattern_detection_db\",\n",
    "    user=\"postgres\",\n",
    "    password=\"rdspostgres\"\n",
    ")\n",
    "pg_cursor = pg_conn.cursor()\n",
    "\n",
    "# ------------------ Transaction Schema ------------------\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"zipcodeOri\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"zipMerchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "pg_cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS processed_chunks (\n",
    "    chunk_folder TEXT PRIMARY KEY\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "pg_cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pattern1_customer_merchant_counts (\n",
    "    customer_name TEXT,\n",
    "    merchant_id TEXT,\n",
    "    txn_count BIGINT,\n",
    "    detected BOOLEAN DEFAULT FALSE,\n",
    "    PRIMARY KEY (customer_name, merchant_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "pg_cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pattern2_customer_merchant_stats (\n",
    "    customer_name TEXT,\n",
    "    merchant_id TEXT,\n",
    "    txn_count BIGINT,\n",
    "    total_amount DOUBLE PRECISION,\n",
    "    avg_amount DOUBLE PRECISION,\n",
    "    detected BOOLEAN DEFAULT FALSE,\n",
    "    PRIMARY KEY (customer_name, merchant_id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "pg_cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pattern3_merchant_gender_stats (\n",
    "    merchant_id TEXT PRIMARY KEY,\n",
    "    male_count BIGINT DEFAULT 0,\n",
    "    female_count BIGINT DEFAULT 0,\n",
    "    detected BOOLEAN DEFAULT FALSE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "pg_conn.commit()\n",
    "\n",
    "\n",
    "def list_available_chunks():\n",
    "    print(\"Listing available chunks from S3...\")\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=input_prefix, Delimiter=\"/\")\n",
    "    return [obj[\"Prefix\"] for obj in response.get(\"CommonPrefixes\", [])]\n",
    "\n",
    "def is_chunk_processed(folder_key):\n",
    "    pg_cursor.execute(\"SELECT 1 FROM processed_chunks WHERE chunk_folder = %s\", (folder_key,))\n",
    "    return pg_cursor.fetchone() is not None\n",
    "\n",
    "def mark_chunk_processed(folder_key):\n",
    "    pg_cursor.execute(\"INSERT INTO processed_chunks (chunk_folder) VALUES (%s) ON CONFLICT DO NOTHING\", (folder_key,))\n",
    "    pg_conn.commit()\n",
    "\n",
    "# ------------------ Pattern 1 ------------------\n",
    "def update_stats_pattern1(df):\n",
    "    print(\"Updating Pattern 1 stats...\")\n",
    "    counts_df = df.groupBy(\"merchant\", \"customer\").agg(count(\"*\").alias(\"txn_count\"))\n",
    "    for row in counts_df.toLocalIterator():\n",
    "        pg_cursor.execute(\"\"\"\n",
    "            INSERT INTO pattern1_customer_merchant_counts (customer_name, merchant_id, txn_count)\n",
    "            VALUES (%s, %s, %s)\n",
    "            ON CONFLICT (customer_name, merchant_id) DO UPDATE SET\n",
    "                txn_count = pattern1_customer_merchant_counts.txn_count + EXCLUDED.txn_count\n",
    "        \"\"\", (row[\"customer\"], row[\"merchant\"], row[\"txn_count\"]))\n",
    "    pg_conn.commit()\n",
    "\n",
    "def detect_pattern1():\n",
    "    print(\"Detecting Pattern 1...\")\n",
    "    pg_cursor.execute(\"\"\"\n",
    "        SELECT merchant_id, SUM(txn_count) as total_txns\n",
    "        FROM pattern1_customer_merchant_counts\n",
    "        GROUP BY merchant_id\n",
    "        HAVING SUM(txn_count) > 500\n",
    "    \"\"\")\n",
    "    merchants = pg_cursor.fetchall()\n",
    "    detections = []\n",
    "    for merchant_id, _ in merchants:\n",
    "        pg_cursor.execute(\"\"\"\n",
    "            SELECT customer_name, txn_count FROM pattern1_customer_merchant_counts\n",
    "            WHERE merchant_id = %s AND detected = FALSE\n",
    "            ORDER BY txn_count DESC\n",
    "        \"\"\", (merchant_id,))\n",
    "        rows = pg_cursor.fetchall()\n",
    "        if not rows:\n",
    "            continue\n",
    "        top_10_index = max(1, len(rows) // 10)\n",
    "        for row in rows[:top_10_index]:\n",
    "            detections.append((row[0], merchant_id))\n",
    "    return format_detection_rows(detections, \"PatId1\", \"UPGRADE\")\n",
    "\n",
    "# ------------------ Pattern 2 ------------------\n",
    "def update_stats_pattern2(df):\n",
    "    print(\"Updating Pattern 2 stats...\")\n",
    "    stats_df = df.groupBy(\"merchant\", \"customer\").agg(\n",
    "        count(\"*\").alias(\"txn_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    )\n",
    "    for row in stats_df.toLocalIterator():\n",
    "        total_amt = row[\"avg_amount\"] * row[\"txn_count\"]\n",
    "        pg_cursor.execute(\"\"\"\n",
    "            INSERT INTO pattern2_customer_merchant_stats (customer_name, merchant_id, txn_count, total_amount, avg_amount)\n",
    "            VALUES (%s, %s, %s, %s, %s)\n",
    "            ON CONFLICT (customer_name, merchant_id) DO UPDATE SET\n",
    "                txn_count = pattern2_customer_merchant_stats.txn_count + EXCLUDED.txn_count,\n",
    "                total_amount = pattern2_customer_merchant_stats.total_amount + EXCLUDED.total_amount,\n",
    "                avg_amount = (pattern2_customer_merchant_stats.total_amount + EXCLUDED.total_amount) /\n",
    "                             (pattern2_customer_merchant_stats.txn_count + EXCLUDED.txn_count)\n",
    "        \"\"\", (row[\"customer\"], row[\"merchant\"], row[\"txn_count\"], total_amt, row[\"avg_amount\"]))\n",
    "    pg_conn.commit()\n",
    "\n",
    "def detect_pattern2():\n",
    "    print(\"Detecting Pattern 2...\")\n",
    "    pg_cursor.execute(\"\"\"\n",
    "        SELECT customer_name, merchant_id\n",
    "        FROM pattern2_customer_merchant_stats\n",
    "        WHERE txn_count > 1 AND (total_amount / txn_count) < 800 AND detected = FALSE\n",
    "    \"\"\")\n",
    "    rows = pg_cursor.fetchall()\n",
    "    return format_detection_rows(rows, \"PatId2\", \"CHILD\")\n",
    "\n",
    "# ------------------ Pattern 3 ------------------\n",
    "def update_stats_pattern3(df):\n",
    "    print(\"Updating Pattern 3 stats...\")\n",
    "    gender_counts = df.groupBy(\"merchant\", \"gender\").count().toLocalIterator()\n",
    "    for row in gender_counts:\n",
    "        if row[\"gender\"] == \"F\":\n",
    "            pg_cursor.execute(\"\"\"\n",
    "                INSERT INTO pattern3_merchant_gender_stats (merchant_id, female_count)\n",
    "                VALUES (%s, %s)\n",
    "                ON CONFLICT (merchant_id) DO UPDATE\n",
    "                SET female_count = pattern3_merchant_gender_stats.female_count + EXCLUDED.female_count\n",
    "            \"\"\", (row[\"merchant\"], row[\"count\"]))\n",
    "        elif row[\"gender\"] == \"M\":\n",
    "            pg_cursor.execute(\"\"\"\n",
    "                INSERT INTO pattern3_merchant_gender_stats (merchant_id, male_count)\n",
    "                VALUES (%s, %s)\n",
    "                ON CONFLICT (merchant_id) DO UPDATE\n",
    "                SET male_count = pattern3_merchant_gender_stats.male_count + EXCLUDED.male_count\n",
    "            \"\"\", (row[\"merchant\"], row[\"count\"]))\n",
    "    pg_conn.commit()\n",
    "\n",
    "def detect_pattern3():\n",
    "    print(\"Detecting Pattern 3...\")\n",
    "    pg_cursor.execute(\"\"\"\n",
    "        SELECT merchant_id FROM pattern3_merchant_gender_stats\n",
    "        WHERE female_count > 10 AND female_count < male_count AND detected = FALSE\n",
    "    \"\"\")\n",
    "    rows = pg_cursor.fetchall()\n",
    "    return format_detection_rows([(None, row[0]) for row in rows], \"PatId3\", \"DEI-NEEDED\")\n",
    "\n",
    "\n",
    "def format_detection_rows(rows, pattern_id, action_type):\n",
    "    schema = StructType([\n",
    "        StructField(\"YStartTime\", StringType(), True),\n",
    "        StructField(\"detectionTime\", TimestampType(), True),\n",
    "        StructField(\"patternId\", StringType(), True),\n",
    "        StructField(\"actionType\", StringType(), True),\n",
    "        StructField(\"customerName\", StringType(), True),\n",
    "        StructField(\"merchantId\", StringType(), True),\n",
    "    ])\n",
    "    \n",
    "    if not rows:\n",
    "        print(f\">>> No rows to format for {pattern_id}\")\n",
    "        return spark.createDataFrame([], schema=schema)\n",
    "    \n",
    "    formatted = [(y_start_time, None, pattern_id, action_type, row[0] or \"\", row[1] or \"\") for row in rows]\n",
    "    return spark.createDataFrame(formatted, schema=schema).withColumn(\"detectionTime\", current_timestamp())\n",
    "\n",
    "\n",
    "def process_chunk(folder_key):\n",
    "    print(f\"\\n Processing chunk: {folder_key} ---\")\n",
    "\n",
    "    if is_chunk_processed(folder_key):\n",
    "        print(f\"Already processed: {folder_key}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_chunk = (\n",
    "            spark.read\n",
    "            .option(\"header\", True)\n",
    "            .schema(transaction_schema)\n",
    "            .csv(f\"s3a://{bucket_name}/{folder_key}\")\n",
    "        )\n",
    "\n",
    "        if df_chunk.rdd.isEmpty():\n",
    "            print(f\"Chunk {folder_key} is empty. Skipping.\")\n",
    "            mark_chunk_processed(folder_key)\n",
    "            return\n",
    "\n",
    "        print(\"Updating pattern statistics...\")\n",
    "        update_stats_pattern1(df_chunk)\n",
    "        update_stats_pattern2(df_chunk)\n",
    "        update_stats_pattern3(df_chunk)\n",
    "\n",
    "        # Detect patterns\n",
    "        detections1 = detect_pattern1().filter(col(\"customerName\") != \"\").filter(col(\"merchantId\") != \"\")\n",
    "        detections2 = detect_pattern2().filter(col(\"customerName\") != \"\").filter(col(\"merchantId\") != \"\")\n",
    "        detections3 = detect_pattern3().filter(col(\"merchantId\") != \"\")  # PatId3 may not have customerName\n",
    "\n",
    "        # For Debugging\n",
    "        if not detections1.rdd.isEmpty():\n",
    "            print(\">>> Pattern 1 detections (up to 5 rows):\")\n",
    "            detections1.show(5, truncate=False)\n",
    "        else:\n",
    "            print(\">>> Pattern 1: No detections found.\")\n",
    "\n",
    "        if not detections2.rdd.isEmpty():\n",
    "            print(\">>> Pattern 2 detections (up to 5 rows):\")\n",
    "            detections2.show(5, truncate=False)\n",
    "        else:\n",
    "            print(\">>> Pattern 2: No detections found.\")\n",
    "\n",
    "        if not detections3.rdd.isEmpty():\n",
    "            print(\">>> Pattern 3 detections (up to 5 rows):\")\n",
    "            detections3.show(5, truncate=False)\n",
    "        else:\n",
    "            print(\">>> Pattern 3: No detections found.\")\n",
    "\n",
    "        detections = detections1.unionByName(detections2).unionByName(detections3)\n",
    "\n",
    "        if detections.rdd.isEmpty():\n",
    "            print(f\"No patterns detected in {folder_key}\")\n",
    "            mark_chunk_processed(folder_key)\n",
    "            return\n",
    "\n",
    "        print(f\"Total detections in {folder_key}: {detections.count()}\")\n",
    "\n",
    "        # Batch and write detections to S3\n",
    "        detections = detections.withColumn(\"row_id\", row_number().over(Window.orderBy(\"customerName\", \"merchantId\")))\n",
    "        total = detections.count()\n",
    "        batch_count = (total + 99) // 100\n",
    "\n",
    "        for batch_num in range(batch_count):\n",
    "            batch_df = detections.filter(\n",
    "                (col(\"row_id\") > batch_num * 50) & (col(\"row_id\") <= (batch_num + 1) * 50)\n",
    "            ).drop(\"row_id\")\n",
    "\n",
    "            timestamp = int(time.time())\n",
    "            output_path = f\"s3a://{bucket_name}/{output_prefix}pattern_detected_{timestamp}_{batch_num}.csv\"\n",
    "            print(f\"Writing batch {batch_num + 1}/{batch_count} to {output_path}\")\n",
    "            batch_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path)\n",
    "\n",
    "            # --- Update detected flags in DB ---\n",
    "            for row in batch_df.select(\"patternId\", \"customerName\", \"merchantId\").toLocalIterator():\n",
    "                if row[\"patternId\"] == \"PatId1\":\n",
    "                    pg_cursor.execute(\"\"\"\n",
    "                        UPDATE pattern1_customer_merchant_counts\n",
    "                        SET detected = TRUE\n",
    "                        WHERE customer_name = %s AND merchant_id = %s\n",
    "                    \"\"\", (row[\"customerName\"], row[\"merchantId\"]))\n",
    "                elif row[\"patternId\"] == \"PatId2\":\n",
    "                    pg_cursor.execute(\"\"\"\n",
    "                        UPDATE pattern2_customer_merchant_stats\n",
    "                        SET detected = TRUE\n",
    "                        WHERE customer_name = %s AND merchant_id = %s\n",
    "                    \"\"\", (row[\"customerName\"], row[\"merchantId\"]))\n",
    "                elif row[\"patternId\"] == \"PatId3\":\n",
    "                    pg_cursor.execute(\"\"\"\n",
    "                        UPDATE pattern3_merchant_gender_stats\n",
    "                        SET detected = TRUE\n",
    "                        WHERE merchant_id = %s\n",
    "                    \"\"\", (row[\"merchantId\"],))\n",
    "\n",
    "            pg_conn.commit()\n",
    "\n",
    "        mark_chunk_processed(folder_key)\n",
    "        print(f\"Finished processing chunk: {folder_key}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {folder_key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b925235-8285-4c2e-9516-3f4e91f74c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    chunk_folders = list_available_chunks()\n",
    "    \n",
    "    for folder_key in chunk_folders:\n",
    "        process_chunk(folder_key)\n",
    "\n",
    "main()  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pattern_detection_Mechanism_Y",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
